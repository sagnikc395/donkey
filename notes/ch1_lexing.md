- we need to do is repr our source code in other forms that are eaier to work with.![[Screenshot 2024-02-22 at 12.41.01â€¯AM.png]]
- This transformation from source code to tokens is called as lexical analysis or lexing for short. Its done by a lexer.
- Tokens are itself are small,easily categorizable data structures that are then fed to the parser , which does the second transformation and turns the tokens into an AST.
- whitespaces characters dont show up as tokens. In our case , that's okay, because whitespace length is not significant in Monkey.
- TokenType type is defined as a string. Allows us to use many different values as TokenType , which in turn allows us to distinguish between different types of tokens. Using string also has the advantages of being easy to debug without a lot of boilerplate and helper functions: we can just print a string.
- we also defines the constants for the limited number of different token types in the Monkey language.
- Illegal signifies a token/character we dont know about and EOF stands for End of file which tells our parser later on that it can stop.

## The Lexer:
- Take source code as input and output the tokens that represent the source code. It will go through its input and output the next token it recognizes. It doesnt need to buffer or save tokens, since there will be only be one method called NextToken().
- Init the lexer with our source code and then repeatedly call NextToken() on it to go through the source code , token by token , character by character.
- doing a TDD based approach.

```go 
type Lexer struct {
	input string 
	position int 
	readPosition int 
	ch byte 
}
```
- The reason for these 2 pointers pointing into our input string is the fact that we will need to be able to peek further into the input and look after the current character to see what comes up next.
- readPosition always points to the "next" character in the input. position points to the character in the input that corresponds to the ch byte.
- readChar() method will read the character at the current posn.
	- is to give us the next character and advance our position in the input string.
	- The first thing it checks whether we have reached the end of input.
	- l.readPosition always points to the next position where we are going to read from next and l.position always points to the position where we last read.
- Currently this only supports ASCII character , to fully support Unicode and UTF-8 , we would need to change l.ch from byte to rune and also change the way we read the next characters, since they could be multiple bytes wide now.
- NextToken() method:
	- look at the current character under examination (l.ch) and return a token depending on which character it is.
	- Before returning the token we advanced our pointers into the input so when we call NextToken() again the l.ch filed is already updated.
	- newToken() helps us with initializing these tokens.

### Identifiers and Keywords:
- What the lexer needs to do is recognize whether the current character is a a letter and if so, it needs to read the rest of the identifier/keyword until it encounters a non-letter-character. 
- Having read that identifier/keyword , we then need to find out if it is a identifier or a keyword , so we can use the correct token.TokenType.
- To add Type Information about Literal field of our current token we need to be able to tell user-defined identifiers apart from language keywords. We add LookupIdent as a map lookup for our keywords and returns the current TokenType for the token literal we have.
	- LookupIdent checks the keywords table to see whether the given identifier is in fact a keyword. If it is, it will return the keyword's TokenType constant. If it isnt , we just get back token.IDENT, which is the TokenType for all user-defined identifiers.
	- Also remove whitespace in tokens ; as in Monkey, whitespace only acts as a separator of tokens and doesnt have a meaning so we need to skip it entirely.

## Extending Token Set and Lexer:
- To eliminate jumping between packages when writing our parser, we need to extend our lexer so it can recognize more of the Monkey lang and output more tokens.
- the new tokens can be classified into 3 categroies:
	- 1 character long 
	- 2 character long 
	- keyword token
- Support for \[-,\/,*,<,>] is tirivial.
- For these add the tokens in the map and check the tests.
- Supporting 2 characters tokens to our lexer:
	- Extend the exisiting brances for = and ! and extend them. So we look ahead in the input and then determine whether to return a token for = or == .
	- Adding a peekChar() method that doesnt increment l.position and l.readPosition. We only want to "peek" ahead in the input and not move around in it, so we know that a call to readChar() would return.
	- Difficulty of parsing different languages often comes down to how far or how before we need to peek in source code to make sense of it.
	- When the lexer comes to == , in input, it creates 2 token.ASSIGN tokens instead of one token.EQ token.
		- Soln: use our new peekChar() method in the branches of the switch statement for '=' and ! as we peek ahead. If the next tokens are also = , we create either a token.EQ or token.NOT_EQ.
### Repl:
- REPL stands for Read Eval Print Loop.
- Supported in most langauges.
- REPL sometimes called console, sometimes interactive mode.
- REPL reads the input, sends it to the interpreter for evaluation, prints the result/ output of the interpreter and starts again.
- read from the input source until encountering a newline, take the just read line and pass it to an instance of our lexer and finally print all the tokens the lexer gives us until we encounter EOF.